<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Logistic Regression Algorithm</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #2980b9;
        }
        code {
            background-color: #f8f9fa;
            padding: 2px 4px;
            border-radius: 4px;
            font-family: monospace;
        }
        pre {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        .note {
            background-color: #e7f5fe;
            border-left: 4px solid #3498db;
            padding: 10px;
            margin: 15px 0;
        }
        .equation {
            font-family: serif;
            text-align: center;
            margin: 15px 0;
        }
        .comparison {
            background-color: #f0f7f4;
            border-left: 4px solid #2ecc71;
            padding: 10px;
            margin: 15px 0;
        }
    </style>
</head>
<body>
    <h1>Understanding Logistic Regression Algorithm</h1>
    
    <section>
        <h2>What is Logistic Regression?</h2>
        <p>
            Logistic regression is a statistical method for analyzing datasets where the outcome variable is binary or categorical (e.g., yes/no, true/false, 0/1). 
            Despite its name, it's actually a classification algorithm, not a regression algorithm.
        </p>
        
        <div class="note">
            <strong>Key Characteristics:</strong>
            <ul>
                <li>Used for binary classification problems (can be extended to multi-class)</li>
                <li>Outputs probabilities between 0 and 1 using the logistic (sigmoid) function</li>
                <li>Works well when there's approximately a linear relationship between features and log-odds</li>
                <li>Provides interpretable results through coefficients</li>
            </ul>
        </div>
        
        <div class="comparison">
            <strong>Logistic vs Linear Regression:</strong>
            <ul>
                <li><strong>Linear regression</strong> predicts continuous values</li>
                <li><strong>Logistic regression</strong> predicts probabilities for discrete classes</li>
                <li>Logistic regression uses the sigmoid function to constrain outputs between 0 and 1</li>
            </ul>
        </div>
    </section>
    
    <section>
        <h2>The Logistic Regression Equation</h2>
        <p>
            The logistic regression model uses the logistic (sigmoid) function to transform linear regression outputs into probabilities:
        </p>
        
        <div class="equation">
            p = 1 / (1 + e<sup>-(β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ)</sup>)
        </div>
        
        <p>Where:</p>
        <ul>
            <li><strong>p</strong> is the probability of the outcome being 1</li>
            <li><strong>e</strong> is the base of natural logarithms (~2.718)</li>
            <li><strong>β₀</strong> is the intercept term</li>
            <li><strong>β₁ to βₙ</strong> are the coefficients for each feature</li>
            <li><strong>x₁ to xₙ</strong> are the input features</li>
        </ul>
        
        <p>The log-odds (logit) is the linear combination of the features:</p>
        
        <div class="equation">
            log(p/(1-p)) = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ
        </div>
    </section>
    
    <section>
        <h2>How Logistic Regression Works</h2>
        <p>
            Logistic regression estimates the parameters using <strong>maximum likelihood estimation (MLE)</strong> rather than minimizing squared errors like linear regression.
        </p>
        
        <p>The steps involved are:</p>
        <ol>
            <li>Transform the linear output using the sigmoid function to get probabilities</li>
            <li>Calculate the likelihood of the observed data given the parameters</li>
            <li>Adjust parameters to maximize the likelihood function</li>
            <li>Use the optimized parameters to make predictions</li>
            <li>Apply a decision threshold (typically 0.5) to classify observations</li>
        </ol>
        
        <div class="note">
            <strong>Sigmoid Function Properties:</strong>
            <ul>
                <li>Maps any real-valued number to the (0, 1) range</li>
                <li>S-shaped curve</li>
                <li>Output can be interpreted as probability</li>
            </ul>
        </div>
    </section>
    
    <section>
        <h2>Implementing Logistic Regression in Python</h2>
        <p>
            Let's implement logistic regression using Python with <code>scikit-learn</code>, the popular machine learning library.
        </p>
        
        <h3>1. Binary Classification Example</h3>
        <pre><code># Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.datasets import make_classification

# Generate synthetic data
X, y = make_classification(n_samples=100, n_features=1, n_informative=1, 
                           n_redundant=0, n_classes=2, random_state=42)

# Create and fit the model
model = LogisticRegression()
model.fit(X, y)

# Make predictions
y_pred = model.predict(X)
y_prob = model.predict_proba(X)[:, 1]  # Probability estimates

# Model evaluation
print("Coefficient (β₁):", model.coef_[0][0])
print("Intercept (β₀):", model.intercept_[0])
print("\nConfusion Matrix:")
print(confusion_matrix(y, y_pred))
print("\nClassification Report:")
print(classification_report(y, y_pred))
print("Accuracy:", accuracy_score(y, y_pred))

# Plotting
plt.scatter(X, y, color='blue', label='Actual data')
x_values = np.linspace(-3, 3, 300)
y_values = model.predict_proba(x_values.reshape(-1, 1))[:, 1]
plt.plot(x_values, y_values, color='red', label='Logistic curve')
plt.xlabel('Feature')
plt.ylabel('Probability/Class')
plt.title('Logistic Regression')
plt.legend()
plt.show()</code></pre>
        
        <h3>2. Multi-class Classification Example</h3>
        <pre><code># Import libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and fit the model
# multi_class='multinomial' enables softmax regression for multi-class
model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)

# Model evaluation
print("Coefficients:\n", model.coef_)
print("\nIntercepts:", model.intercept_)
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))</code></pre>
    </section>
    
    <section>
        <h2>Assumptions of Logistic Regression</h2>
        <p>
            Logistic regression makes several key assumptions:
        </p>
        <ul>
            <li><strong>Binary outcome:</strong> The dependent variable should be binary (for standard logistic regression)</li>
            <li><strong>Independence:</strong> Observations should be independent of each other</li>
            <li><strong>Linearity:</strong> There should be a linear relationship between the logit of the outcome and each predictor</li>
            <li><strong>No multicollinearity:</strong> Predictors should not be highly correlated with each other</li>
            <li><strong>Large sample size:</strong> Requires sufficient cases for each predictor category</li>
        </ul>
    </section>
    
    <section>
        <h2>Advantages and Limitations</h2>
        
        <h3>Advantages:</h3>
        <ul>
            <li>Outputs probabilities, not just class predictions</li>
            <li>Easy to implement and interpret</li>
            <li>Performs well when the dataset is linearly separable</li>
            <li>Less prone to overfitting with low-dimensional data</li>
            <li>Can be regularized to avoid overfitting</li>
        </ul>
        
        <h3>Limitations:</h3>
        <ul>
            <li>Assumes linear decision boundary</li>
            <li>Struggles with complex nonlinear relationships</li>
            <li>Sensitive to outliers</li>
            <li>Requires features to be relatively independent</li>
            <li>Needs large sample size for stable results</li>
        </ul>
    </section>
    
    <section>
        <h2>Practical Applications</h2>
        <p>
            Logistic regression is widely used in various domains:
        </p>
        <ul>
            <li><strong>Healthcare:</strong> Disease prediction (diabetes, heart disease)</li>
            <li><strong>Finance:</strong> Credit scoring, fraud detection</li>
            <li><strong>Marketing:</strong> Customer churn prediction, response modeling</li>
            <li><strong>Social Sciences:</strong> Voting behavior analysis</li>
            <li><strong>Image Processing:</strong> Binary image classification</li>
        </ul>
    </section>
    
    <section>
        <h2>Conclusion</h2>
        <p>
            Logistic regression is a fundamental classification algorithm that provides both probabilities and class predictions. 
            Its simplicity and interpretability make it a valuable first approach for binary classification problems.
        </p>
        <p>
            The Python implementations demonstrate how to apply logistic regression to both binary and multi-class problems 
            using scikit-learn. While it has limitations, especially with complex nonlinear relationships, it remains 
            one of the most widely used algorithms in machine learning due to its efficiency and interpretability.
        </p>
        <p>
            For improved performance, consider feature engineering, regularization techniques (L1/L2), or trying more complex 
            algorithms if logistic regression underperforms on your specific problem.
        </p>
    </section>
</body>
</html>