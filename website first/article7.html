<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decision Tree Classification: Complete Guide</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            max-width: 1100px;
            margin: 0 auto;
            padding: 30px;
            color: #333;
            background-color: #f8f9fa;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #27ae60;
            padding-bottom: 15px;
            text-align: center;
            margin-bottom: 35px;
        }
        h2 {
            color: #27ae60;
            margin-top: 40px;
            border-left: 4px solid #2ecc71;
            padding-left: 15px;
        }
        h3 {
            color: #16a085;
            margin-top: 30px;
        }
        code {
            background-color: #e8f8f5;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Roboto Mono', monospace;
            font-size: 0.95em;
            color: #c0392b;
        }
        pre {
            background-color: #e8f8f5;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            border-left: 4px solid #2ecc71;
            box-shadow: 0 3px 6px rgba(0,0,0,0.05);
            line-height: 1.5;
            font-size: 0.95em;
        }
        .note {
            background-color: #e8f8f5;
            border-left: 4px solid #2ecc71;
            padding: 18px;
            margin: 30px 0;
            border-radius: 0 8px 8px 0;
        }
        .equation {
            font-family: 'Cambria', Georgia, serif;
            text-align: center;
            margin: 25px 0;
            font-size: 1.2em;
            color: #2c3e50;
            padding: 15px;
            background-color: #e8f8f5;
            border-radius: 8px;
            border: 1px solid #d5f5e3;
        }
        .image-container {
            text-align: center;
            margin: 35px 0;
            padding: 20px;
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 4px 10px rgba(0,0,0,0.08);
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 6px;
        }
        .image-caption {
            font-style: italic;
            text-align: center;
            margin-top: 12px;
            color: #7f8c8d;
            font-size: 0.95em;
        }
        .grid-container {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 25px;
            margin: 35px 0;
        }
        .card {
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 3px 8px rgba(0,0,0,0.08);
            border-top: 3px solid #2ecc71;
        }
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 35px 0;
            box-shadow: 0 2px 6px rgba(0,0,0,0.08);
        }
        .comparison-table th, .comparison-table td {
            padding: 16px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        .comparison-table th {
            background-color: #27ae60;
            color: white;
            font-weight: 600;
        }
        .comparison-table tr:nth-child(even) {
            background-color: #f5fbf8;
        }
        .comparison-table tr:hover {
            background-color: #e8f8f5;
        }
        .algorithm-steps {
            background-color: #e8f8f5;
            padding: 20px;
            border-radius: 8px;
            margin: 25px 0;
            border-left: 4px solid #27ae60;
        }
        .split-method {
            background-color: #f5fbf8;
            padding: 15px;
            border-radius: 6px;
            margin: 20px 0;
            border: 1px solid #d5f5e3;
        }
    </style>
</head>
<body>
    <h1>Decision Tree Classification</h1>
    
    <section>
        <h2>Introduction to Decision Trees</h2>
        <p>
            Decision Trees are versatile supervised learning algorithms that can perform both classification and regression tasks. 
            They work by recursively partitioning the feature space into regions, with each partition represented as a node in the tree.
        </p>
        
        <div class="image-container">
            <img src="https://www.researchgate.net/publication/334377454/figure/fig1/AS:781538235949057@1563390754820/Decision-Tree-structure-and-terminology.png" alt="Decision Tree Structure">
            <div class="image-caption">Figure 1: Decision Tree structure and terminology</div>
        </div>
        
        <div class="note">
            <strong>Key Characteristics:</strong>
            <ul>
                <li>Non-parametric method (no assumptions about data distribution)</li>
                <li>Can handle both numerical and categorical data</li>
                <li>Easy to interpret and visualize</li>
                <li>Can model non-linear relationships</li>
                <li>Prone to overfitting without proper regularization</li>
            </ul>
        </div>
    </section>
    
    <section>
        <h2>How Decision Trees Work</h2>
        <p>
            Decision trees learn by recursively splitting the dataset into subsets based on feature values. The algorithm selects the best split at each node 
            according to a specified criterion, continuing until a stopping condition is met.
        </p>
        
        <div class="algorithm-steps">
            <h3>Decision Tree Learning Algorithm:</h3>
            <ol>
                <li><strong>Start</strong> at the root node with the entire dataset</li>
                <li><strong>Select the best feature</strong> to split the data (using impurity measure)</li>
                <li><strong>Create decision nodes</strong> for each possible value of the selected feature</li>
                <li><strong>Recurse</strong> on each subset until stopping criteria are met:
                    <ul>
                        <li>All samples belong to one class</li>
                        <li>Maximum tree depth reached</li>
                        <li>Minimum samples per leaf node</li>
                        <li>No improvement in impurity</li>
                    </ul>
                </li>
                <li><strong>Assign leaf nodes</strong> with the majority class of their samples</li>
            </ol>
        </div>
        
        <div class="image-container">
            <img src="https://www.researchgate.net/publication/334377454/figure/fig2/AS:781538235949058@1563390754822/Decision-Tree-splitting-process.png" alt="Decision Tree Splitting">
            <div class="image-caption">Figure 2: Decision Tree recursive partitioning process</div>
        </div>
    </section>
    
    <section>
        <h2>Splitting Criteria</h2>
        <p>
            The "best" split is determined by measuring the impurity of the resulting subsets. Common impurity measures for classification include:
        </p>
        
        <div class="grid-container">
            <div class="split-method">
                <h3>1. Gini Impurity</h3>
                <div class="equation">
                    Gini = 1 - Σ(pᵢ²)
                </div>
                <p>Where pᵢ is the proportion of class i in the node.</p>
                <ul>
                    <li>Measures probability of incorrect classification</li>
                    <li>Ranges from 0 (perfect purity) to 0.5 (max impurity for binary classification)</li>
                    <li>Default in scikit-learn's DecisionTreeClassifier</li>
                </ul>
            </div>
            
            <div class="split-method">
                <h3>2. Information Gain (Entropy)</h3>
                <div class="equation">
                    Entropy = -Σ(pᵢ × log₂(pᵢ))
                </div>
                <p>Where pᵢ is the proportion of class i in the node.</p>
                <ul>
                    <li>Measures disorder/uncertainty</li>
                    <li>Ranges from 0 (perfect purity) to 1 (max impurity for binary classification)</li>
                    <li>Tends to produce more balanced trees</li>
                </ul>
            </div>
        </div>
        
        <div class="note">
            <strong>Choosing Between Gini and Entropy:</strong>
            <ul>
                <li>Gini is slightly faster to compute</li>
                <li>Both usually produce similar results</li>
                <li>Gini may be more sensitive to class probability changes</li>
                <li>Entropy may produce slightly more balanced trees</li>
            </ul>
        </div>
    </section>
    
    <section>
        <h2>Implementing Decision Tree in Python</h2>
        
        <h3>1. Basic Decision Tree Classifier</h3>
        <pre><code># Import required libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Load Iris dataset
iris = load_iris()
X = iris.data[:, :2]  # Use only first two features for visualization
y = iris.target

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create Decision Tree classifier
dt_clf = DecisionTreeClassifier(
    criterion='gini',     # Splitting criterion
    max_depth=3,         # Maximum tree depth
    min_samples_split=2, # Minimum samples to split a node
    min_samples_leaf=1,  # Minimum samples at a leaf node
    random_state=42
)

# Train the model
dt_clf.fit(X_train, y_train)

# Make predictions
y_pred = dt_clf.predict(X_test)

# Evaluate model
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Visualize the decision tree
plt.figure(figsize=(15, 10))
plot_tree(dt_clf, 
          feature_names=iris.feature_names[:2], 
          class_names=iris.target_names,
          filled=True, 
          rounded=True)
plt.title("Decision Tree Visualization")
plt.show()</code></pre>
        
        <div class="image-container">
            <img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dtc_001.png" alt="Decision Tree Visualization">
            <div class="image-caption">Figure 3: Visualized decision tree for Iris dataset classification</div>
        </div>
        
        <h3>2. Visualizing Decision Boundaries</h3>
        <pre><code># Plot decision boundaries
from mlxtend.plotting import plot_decision_regions

plt.figure(figsize=(10, 6))
plot_decision_regions(X_train, y_train, clf=dt_clf, legend=2)
plt.xlabel('Sepal length (cm)')
plt.ylabel('Sepal width (cm)')
plt.title('Decision Tree Decision Boundaries')
plt.show()</code></pre>
        
        <div class="image-container">
            <img src="https://www.researchgate.net/publication/334377454/figure/fig3/AS:781538235949059@1563390754824/Decision-Tree-decision-boundaries.png" alt="Decision Boundaries">
            <div class="image-caption">Figure 4: Decision boundaries created by the decision tree</div>
        </div>
        
        <h3>3. Feature Importance Analysis</h3>
        <pre><code># Train on all features
dt_clf_full = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_clf_full.fit(iris.data, iris.target)

# Get feature importances
importances = dt_clf_full.feature_importances_
feature_names = iris.feature_names

# Print feature importance scores
print("Feature Importances:")
for name, score in zip(feature_names, importances):
    print(f"{name}: {score:.3f}")

# Plot feature importances
plt.figure(figsize=(8, 4))
plt.barh(feature_names, importances)
plt.xlabel('Importance Score')
plt.title('Decision Tree Feature Importances')
plt.show()</code></pre>
        
        <div class="image-container">
            <img src="https://www.researchgate.net/publication/334377454/figure/fig4/AS:781538235949060@1563390754826/Feature-importance-from-Decision-Tree.png" alt="Feature Importance">
            <div class="image-caption">Figure 5: Feature importance scores from the decision tree</div>
        </div>
    </section>
    
    <section>
        <h2>Hyperparameter Tuning</h2>
        <p>
            Properly tuning decision tree hyperparameters is crucial to prevent overfitting and improve generalization:
        </p>
        
        <table class="comparison-table">
            <tr>
                <th>Parameter</th>
                <th>Description</th>
                <th>Effect</th>
                <th>Recommended Values</th>
            </tr>
            <tr>
                <td>max_depth</td>
                <td>Maximum depth of the tree</td>
                <td>Controls tree complexity</td>
                <td>3-10 (None for unlimited)</td>
            </tr>
            <tr>
                <td>min_samples_split</td>
                <td>Minimum samples to split a node</td>
                <td>Prevents overfitting on small groups</td>
                <td>2-20</td>
            </tr>
            <tr>
                <td>min_samples_leaf</td>
                <td>Minimum samples at a leaf node</td>
                <td>Smooths decision boundaries</td>
                <td>1-10</td>
            </tr>
            <tr>
                <td>max_features</td>
                <td>Number of features to consider for splits</td>
                <td>Adds randomness, can improve generalization</td>
                <td>'sqrt', 'log2', or integer</td>
            </tr>
            <tr>
                <td>criterion</td>
                <td>Splitting quality measure</td>
                <td>Affects how splits are chosen</td>
                <td>'gini' or 'entropy'</td>
            </tr>
        </table>
        
        <div class="note">
            <strong>Tuning Tips:</strong>
            <ul>
                <li>Start with default values and adjust based on validation performance</li>
                <li>Use cross-validation to evaluate different parameter combinations</li>
                <li>Balance between model complexity and performance</li>
                <li>Consider using GridSearchCV for automated parameter search</li>
            </ul>
        </div>
    </section>
    
    <section>
        <h2>Advantages and Limitations</h2>
        
        <div class="grid-container">
            <div class="card">
                <h3>Advantages</h3>
                <ul>
                    <li>Easy to understand and interpret (white box model)</li>
                    <li>Requires little data preprocessing (handles mixed data types)</li>
                    <li>Can handle both classification and regression tasks</li>
                    <li>Non-parametric (no assumptions about data distribution)</li>
                    <li>Feature selection is automatic (through importance scores)</li>
                </ul>
            </div>
            
            <div class="card">
                <h3>Limitations</h3>
                <ul>
                    <li>Prone to overfitting, especially with deep trees</li>
                    <li>Can be unstable (small changes in data can lead to different trees)</li>
                    <li>Biased toward features with more levels</li>
                    <li>Not ideal for extrapolation (predicting outside training range)</li>
                    <li>May create over-complex trees that don't generalize well</li>
                </ul>
            </div>
        </div>
        
        <div class="image-container">
            <img src="https://www.researchgate.net/publication/334377454/figure/fig5/AS:781538235949061@1563390754828/Pros-and-cons-of-Decision-Trees.png" alt="Pros and Cons">
            <div class="image-caption">Figure 6: Summary of Decision Tree advantages and disadvantages</div>
        </div>
    </section>
    
    <section>
        <h2>Practical Applications</h2>
        
        <div class="grid-container">
            <div class="card">
                <h3>1. Medical Diagnosis</h3>
                <ul>
                    <li>Disease prediction based on symptoms</li>
                    <li>Patient risk stratification</li>
                    <li>Treatment effectiveness analysis</li>
                </ul>
            </div>
            
            <div class="card">
                <h3>2. Financial Analysis</h3>
                <ul>
                    <li>Credit scoring</li>
                    <li>Fraud detection</li>
                    <li>Investment decision making</li>
                </ul>
            </div>
            
            <div class="card">
                <h3>3. Customer Relationship</h3>
                <ul>
                    <li>Customer churn prediction</li>
                    <li>Targeted marketing</li>
                    <li>Customer segmentation</li>
                </ul>
            </div>
            
            <div class="card">
                <h3>4. Industrial Applications</h3>
                <ul>
                    <li>Quality control</li>
                    <li>Fault diagnosis</li>
                    <li>Predictive maintenance</li>
                </ul>
            </div>
        </div>
    </section>
    
    <section>
        <h2>Conclusion</h2>
        <p>
            Decision Trees are powerful, intuitive machine learning algorithms that form the foundation for more advanced techniques like Random Forests and Gradient Boosted Trees. 
            Their ability to handle both numerical and categorical data, along with their transparent decision-making process, makes them valuable for many real-world applications.
        </p>
        <p>
            The Python implementation demonstrates how to build, visualize, and interpret decision trees using scikit-learn. Key considerations when using decision trees include 
            proper hyperparameter tuning to control model complexity and prevent overfitting, as well as understanding feature importance to gain insights into the decision-making process.
        </p>
        <p>
            While decision trees have limitations, particularly with overfitting, they remain one of the most interpretable machine learning methods available. For improved performance, 
            consider ensemble methods that combine multiple decision trees, such as Random Forests or Gradient Boosting Machines.
        </p>
    </section>
</body>
</html>