<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Support Vector Machines (SVM)</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #2980b9;
        }
        code {
            background-color: #f8f9fa;
            padding: 2px 4px;
            border-radius: 4px;
            font-family: monospace;
        }
        pre {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        .note {
            background-color: #e7f5fe;
            border-left: 4px solid #3498db;
            padding: 10px;
            margin: 15px 0;
        }
        .equation {
            font-family: serif;
            text-align: center;
            margin: 15px 0;
            font-size: 1.1em;
        }
        .comparison {
            background-color: #f0f7f4;
            border-left: 4px solid #2ecc71;
            padding: 10px;
            margin: 15px 0;
        }
        .image-container {
            text-align: center;
            margin: 20px 0;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 5px;
        }
        .image-caption {
            font-style: italic;
            text-align: center;
            margin-top: 5px;
        }
        .grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 15px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <h1>Understanding Support Vector Machines (SVM)</h1>
    
    <section>
        <h2>What are Support Vector Machines?</h2>
        <p>
            Support Vector Machines (SVMs) are powerful supervised learning algorithms used for both classification and regression tasks. 
            SVMs are particularly effective in high-dimensional spaces and are known for their ability to handle complex decision boundaries.
        </p>
        
        <div class="image-container">
            <img src="https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png" alt="SVM Margin Visualization">
            <div class="image-caption">Figure 1: SVM finds the optimal separating hyperplane with maximum margin</div>
        </div>
        
        <div class="note">
            <strong>Key Characteristics:</strong>
            <ul>
                <li>Effective in high-dimensional spaces</li>
                <li>Memory efficient (uses a subset of training points - support vectors)</li>
                <li>Versatile (different kernel functions can be specified)</li>
                <li>Robust against overfitting, especially in high-dimensional space</li>
            </ul>
        </div>
    </section>
    
    <section>
        <h2>How SVM Works</h2>
        <p>
            The core idea of SVM is to find the optimal hyperplane that separates classes with the maximum margin. 
            The vectors (data points) that define this hyperplane are called support vectors.
        </p>
        
        <div class="grid">
            <div>
                <div class="image-container">
                    <img src="https://www.researchgate.net/publication/361798882/figure/fig1/AS:1176177340858370@1657632923845/Illustration-of-the-support-vector-machine-SVM-classification-concept-The-optimal.png" alt="SVM Concept">
                    <div class="image-caption">Figure 2: SVM classification concept</div>
                </div>
            </div>
            <div>
                <div class="image-container">
                    <img src="https://miro.medium.com/v2/resize:fit:1400/1*ZpkLQfNfRQMKy1q0VJ1WJg.png" alt="Support Vectors">
                    <div class="image-caption">Figure 3: Support vectors define the margin</div>
                </div>
            </div>
        </div>
        
        <h3>Mathematical Formulation</h3>
        <p>For a linear SVM, the decision function is:</p>
        <div class="equation">
            f(x) = sign(w·x + b)
        </div>
        <p>Where:</p>
        <ul>
            <li><strong>w</strong> is the weight vector (normal to the hyperplane)</li>
            <li><strong>b</strong> is the bias term</li>
            <li><strong>x</strong> is the input feature vector</li>
        </ul>
        
        <p>The optimization problem is to maximize the margin, which can be formulated as:</p>
        <div class="equation">
            minimize ½||w||² subject to yᵢ(w·xᵢ + b) ≥ 1 for all i
        </div>
    </section>
    
    <section>
        <h2>Kernel Trick</h2>
        <p>
            SVM can solve nonlinear classification problems using the kernel trick, which implicitly maps inputs into high-dimensional feature spaces.
        </p>
        
        <div class="image-container">
            <img src="https://www.researchgate.net/publication/334507310/figure/fig1/AS:781538235949056@1563390754819/Kernel-trick-in-SVM.ppm" alt="Kernel Trick">
            <div class="image-caption">Figure 4: Kernel trick transforms data to higher dimensions</div>
        </div>
        
        <h3>Common Kernel Functions</h3>
        <ul>
            <li><strong>Linear:</strong> K(xᵢ, xⱼ) = xᵢ·xⱼ</li>
            <li><strong>Polynomial:</strong> K(xᵢ, xⱼ) = (γxᵢ·xⱼ + r)<sup>d</sup></li>
            <li><strong>RBF (Gaussian):</strong> K(xᵢ, xⱼ) = exp(-γ||xᵢ - xⱼ||²)</li>
            <li><strong>Sigmoid:</strong> K(xᵢ, xⱼ) = tanh(γxᵢ·xⱼ + r)</li>
        </ul>
    </section>
    
    <section>
        <h2>Implementing SVM in Python</h2>
        <p>
            Let's implement SVM using Python's scikit-learn library with different kernels and visualize the results.
        </p>
        
        <h3>1. Linear SVM for Classification</h3>
        <pre><code># Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.datasets import make_blobs

# Create 40 separable points
X, y = make_blobs(n_samples=40, centers=2, random_state=6)

# Fit the model
clf = svm.SVC(kernel='linear', C=1000)
clf.fit(X, y)

# Plot the decision boundary
plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)

# Plot the decision function
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Create grid to evaluate model
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = clf.decision_function(xy).reshape(XX.shape)

# Plot decision boundary and margins
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], 
           alpha=0.5, linestyles=['--', '-', '--'])
# Plot support vectors
ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], 
           s=100, linewidth=1, facecolors='none', edgecolors='k')
plt.title('Linear SVM Classification')
plt.show()</code></pre>
        
        <div class="image-container">
            <img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_001.png" alt="Linear SVM Output">
            <div class="image-caption">Figure 5: Linear SVM classification result</div>
        </div>
        
        <h3>2. Non-linear SVM with RBF Kernel</h3>
        <pre><code># Non-linear SVM example
from sklearn.datasets import make_circles
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Create non-linearly separable data
X, y = make_circles(n_samples=100, factor=0.3, noise=0.1, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model with RBF kernel
clf = svm.SVC(kernel='rbf', gamma='scale')
clf.fit(X_train, y_train)

# Make predictions
y_pred = clf.predict(X_test)

# Evaluate
print(classification_report(y_test, y_pred))

# Plot decision boundary
plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = clf.decision_function(xy).reshape(XX.shape)

ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], 
           alpha=0.5, linestyles=['--', '-', '--'])
ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], 
           s=100, linewidth=1, facecolors='none', edgecolors='k')
plt.title('RBF Kernel SVM')
plt.show()</code></pre>
        
        <div class="image-container">
            <img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_nonlinear_001.png" alt="RBF Kernel SVM">
            <div class="image-caption">Figure 6: Non-linear classification with RBF kernel</div>
        </div>
    </section>
    
    <section>
        <h2>SVM Hyperparameters</h2>
        <p>
            Tuning these parameters is crucial for SVM performance:
        </p>
        <ul>
            <li><strong>C (Regularization parameter):</strong> Controls trade-off between smooth decision boundary and classifying training points correctly</li>
            <li><strong>Kernel:</strong> Specifies the kernel type to be used (linear, poly, rbf, sigmoid)</li>
            <li><strong>Gamma (for RBF kernel):</strong> Defines how far the influence of a single training example reaches</li>
            <li><strong>Degree (for polynomial kernel):</strong> Degree of the polynomial kernel function</li>
        </ul>
        
        <div class="image-container">
            <img src="https://www.researchgate.net/publication/336402347/figure/fig1/AS:812472659349505@1570719961053/Effect-of-SVM-parameters-on-decision-boundary.png" alt="SVM Parameters Effect">
            <div class="image-caption">Figure 7: Effect of SVM parameters on decision boundary</div>
        </div>
    </section>
    
    <section>
        <h2>Advantages and Limitations</h2>
        
        <div class="grid">
            <div>
                <h3>Advantages:</h3>
                <ul>
                    <li>Effective in high-dimensional spaces</li>
                    <li>Memory efficient (uses only support vectors)</li>
                    <li>Versatile through kernel functions</li>
                    <li>Robust against overfitting (especially with proper regularization)</li>
                    <li>Works well with clear margin of separation</li>
                </ul>
            </div>
            <div>
                <h3>Limitations:</h3>
                <ul>
                    <li>Not suitable for very large datasets (training time increases)</li>
                    <li>Doesn't directly provide probability estimates</li>
                    <li>Performance drops with noisy data/overlapping classes</li>
                    <li>Requires careful tuning of parameters and kernel choice</li>
                    <li>Difficult to interpret with nonlinear kernels</li>
                </ul>
            </div>
        </div>
    </section>
    
    <section>
        <h2>Practical Applications</h2>
        <p>
            SVM is widely used in various domains:
        </p>
        <ul>
            <li><strong>Text Classification:</strong> Spam detection, sentiment analysis</li>
            <li><strong>Image Recognition:</strong> Handwritten digit recognition, face detection</li>
            <li><strong>Bioinformatics:</strong> Protein classification, cancer classification</li>
            <li><strong>Finance:</strong> Stock market forecasting, credit scoring</li>
            <li><strong>Medicine:</strong> Medical diagnosis, disease prediction</li>
        </ul>
        
        <div class="image-container">
            <img src="https://www.researchgate.net/publication/334377454/figure/fig1/AS:779860457308160@1562844659870/SVM-applications-in-bioinformatics.png" alt="SVM Applications">
            <div class="image-caption">Figure 8: SVM applications in bioinformatics</div>
        </div>
    </section>
    
    <section>
        <h2>Conclusion</h2>
        <p>
            Support Vector Machines are powerful machine learning models that can handle both linear and non-linear classification problems through the use of different kernel functions. 
            Their ability to find optimal decision boundaries with maximum margins makes them particularly effective for many real-world problems.
        </p>
        <p>
            While SVMs require careful parameter tuning and may not scale as well as some other algorithms to very large datasets, they remain a valuable tool in the machine learning practitioner's toolkit, 
            especially for problems with clear margins of separation or when working in high-dimensional spaces.
        </p>
        <p>
            The Python implementations demonstrate how to use SVM for both linear and non-linear classification tasks, with visualization of decision boundaries and support vectors. 
            For practical applications, consider using techniques like grid search for hyperparameter optimization and feature scaling to improve SVM performance.
        </p>
    </section>
</body>
</html>